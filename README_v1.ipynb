{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MicroNet Challenge (Team: OSI AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team build a network having `X` accuracy on cifar-100 with `A` parameters and `B` multiply-add operations, achieveing the MicroNet Challenge score of `xx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "The below figure is our proposed architecture for the cifar-100 dataset. The numbers described above the arrows are the shape of each input and output.  \n",
    "Our architecture consists of:  \n",
    "1. Upsample Layer\n",
    "2. Stem_Conv\n",
    "3. 10 $\\times$ MobileNet V2 Convolution Block (MBConvBlock)\n",
    "4. Head_Conv\n",
    "5. Global Average Pooling\n",
    "6. Fully Connected Layer  \n",
    "\n",
    "The details of Stem_Conv, Head_Conv, and MBConvBlock are described below the 'Main network'.\n",
    "* In addition, in MBConvBlock\\[0\\], there is no the first three layers (Expansion_Conv, BatchNorm, Activation Function) in a block since there is no expansion when $e=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/overview_v1.png\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Our Approach Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-0. Configuration\n",
    "* <b>Data & Model precision</b>\n",
    "    * 16 bits\n",
    "* <b>Data (Please refer to `Config/main.json`)</b>\n",
    "    * Dataset: cifar-100\n",
    "    * Batch size: 128\n",
    "    * Train size/Valid size: 50000/0\n",
    "    * Augmentation: \\[random crop 32*32 with padding of 4, random horizontal flip(p=0.5), normalization\\] \\+ (custom) auto augmentation for cifar-100 \\+ Mixup\n",
    "* <b>Model (Please refer to `Config/main.json`)</b>\n",
    "    * Architecture: See `figure 1`\n",
    "    * Activation function: swish (beta=1)\n",
    "    * Batch normalization: ghost batch normalization (splits=4)\n",
    "    * Optimizer: sgd (lr=0.1, weight_decay=1e-5, momentum=0.9)\n",
    "    * Loss function: cross entropy loss with label smoothing (smoothing factor=0.3)\n",
    "    * Learning rate scheduler: cosine annealing scheduler (T_max=1200, without restart)\n",
    "    * Epochs #: 1200\n",
    "* <b>Pruning (Please refer to `Config/pruning.json`)</b>\n",
    "    * Pruning method(one shot/iterative): iterative\n",
    "    * Desired sparsity/Pruning ratio per iteration: 50%/10%\n",
    "    * Epochs # per pruning iteration: 300\n",
    "    * Optimizer: sgd (lr=0.1, weight_decay=1e-5, momuntum=0.9)\n",
    "    * Loss function: cross entropy loss with label smoothing (smoothing factor=0.3)\n",
    "    * Learning rate scheduler: cosine annealing scheduler (T_max=300, without restart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Architecture Search\n",
    "First of all, we search for a baseline architecture suitable for cifar-100 data set based on the [EfficientNet](https://arxiv.org/pdf/1905.11946.pdf) architecture using autoML. The search process is as follows:\n",
    "1. <b>Block arguments search</b>: In this step, we search the number of MBConvBlock, and kernel size(k), stride(s), expansion ratio(e), input channels(i), output channels(o), and squeeze-expansion ratio(se) in each block. From the results of the block arguments search, we find out that the position of the convolutional layer which serves to reduce resolution, or convolutional layer with stride of 2, is a sensitive factor to accuracy. With this inference, after several hand-made experiments, the above architecture is chosen. (Here, the number of channels differs from the above figure because scaling coefficients are not yet considered.)\n",
    "\n",
    "2. <b>Scaling coefficients search</b>: In this step, after block aurgments are decided, we search three coefficients by adjusting available resources: width, depth, and resolution. Actually, we set the depth coefficient as 1 since its slight change gets even worse in terms of score. Therefore, a resolution coefficient is set randomly within a given range according to the available resources, and then a width coefficient is calculated by \\[available resources / resolution coefficient$^2$\\].  From the results of the scaling coefficients search, we find out that a large resolution coefficient make a greater performance improvement than a large width coefficient under our circumstance. As a result, when we set available resources as 2, we get a resolution coefficient of 1.4. Finally, to lighten this model, we decide a width coefficient as 0.9, and adapt these coefficients to the model we've got via block arguments search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Techniques for Improvement\n",
    "* <b>[Auto augmentation](https://arxiv.org/pdf/1805.09501.pdf)</b>: We search 25 sub-policies for cifar-100 data set based on the augmentation search space in `AutoAugment` except `Cutout` and `SamplePairing`. Please refer to `AutoML_autoaug.py` for the process and `data_utils/autoaugment.py` for the policy we've got.\n",
    "* <b>[Mixup](https://arxiv.org/pdf/1710.09412.pdf)</b>: We add a Mixup technique with $\\alpha$ of 1, which is the hyperparameter for beta-distribution, after auto augmentation. It is believed that this augmentation can help inter-exploration between arbitrary two classes.\n",
    "* <b>[No bias decay](https://arxiv.org/pdf/1812.01187.pdf)</b>: We do not apply weight decay regularizer to biases. Since these part has a small percentage of the total, it can make underfitting.\n",
    "* <b>[Swish activation function](https://arxiv.org/pdf/1710.05941.pdf)</b>: We use a <i>Swish</i> activation function with $\\beta$ of 1, which is $x\\times sigmoid(x)$. This activation function is usually interpreted as a self-gate activation.\n",
    "* <b>[Ghost batch normalization](https://arxiv.org/pdf/1705.08741.pdf)</b>: We use ghost batch normalization, where batch is divided into four smaller ghost batch in our case to match the splited batch size to 32, instead of plain batch normalization.\n",
    "* <b>[Label smoothing](https://arxiv.org/pdf/1512.00567.pdf)</b>: We use a label smoothing technique through which the probability of the correct label is assinged as 0.7, and $\\frac{0.3}{99}$ for the others.\n",
    "* <b>[Cosine annealing scheduler](https://arxiv.org/pdf/1608.03983.pdf)</b>: We use cosine annealing scheduler for adaptive learning rate, and set a period of one cycle as the number of epochs. Hence, there is no restart process.\n",
    "\n",
    "The results are as follows. 점진적으로 technique 추가하면서 table로 결과 표기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Pruning\n",
    "After training the main network, we adapt magnitude-based iterative pruning method. We prune 10% from whole weights and repeat 5 times in the same way, and hence 50% of whole parameters are pruned.  \n",
    "The trade-off between accuracy and sparsity is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Early Exiting\n",
    "Although general CNN models have the static computational graph for whole dataset, many samples can be classified with few computations.  \n",
    "By exiting certain samples ealiear, considerable FLOPs can be saved without significant accuracy degradation.  \n",
    "\n",
    "To decide the best trade-off exiting position, we checked out three candidates in our models; MBConvBlock\\[2\\], MBConvBlock\\[3\\], and MBConvBlock\\[4\\].  \n",
    "Considering the trade-off between accuracy and FLOPs, we finally chose MBConvBlock\\[2\\].  \n",
    "\n",
    "To ensure the performance of the main network to be preserved, <b>we freeze the pruned model and update the parameters of early exiting module only</b>.\n",
    "* <b>Data</b>\n",
    "    * Same with the data configuration for the main network\n",
    "    * Excpet not using (custom) auto augment & Mixup\n",
    "* <b>Model</b>\n",
    "    * Same with the optimization configuration for the main network\n",
    "    * Except not using label smoothing, epochs # is 400, and using adapted loss function.\n",
    "    \n",
    ": early exiting 비율 / final 결과 / \n",
    "특정 블럭에서 나왔을 때, 어느정도의 flops compared with~  \n",
    "Note that the results are not obtained from validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124056791"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "63853343 + 60203448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scoring metric\n",
    "| <div style=\"width:70px\">Input</div>      | Operator         |  k  |  s  |  e  |  i  |  o  |  se  | Parameter Storage | MULTI      |  ADD       | Math Operations |\n",
    "| :---                                     | :---:            |:---:|:---:|:---:|:---:|:---:| :---:| :---:             | :---:      | :---:      | :---:           |\n",
    "| $32^{2}\\times3$                          | Upsample(nearest)| -   | -   | -   | -   | -   | -    | 0                 | 11,907     | 0          | 11,907          |\n",
    "| $63^{2}\\times3$                          | Stem\\_Conv2d     | 3   | 2   | -   | 3   | 24  | -    | 648               | 691,920    | 622,728    | 1,314,648       |\n",
    "| $31^{2}\\times24$                         | MBConvBlock\\[0\\] | 3   | 1   | 1   | 24  | 16  | 0.20 | 424               | 669,132    | 584,456    | 1,253,588       |\n",
    "| $31^{2}\\times16$                         | MBConvBlock\\[1\\] | 3   | 1   | 6   | 16  | 24  | 0.20 | 6,192             | 5,170,329  | 4,593,288  | 9,763,617       |\n",
    "| $31^{2}\\times24$                         | MBConvBlock\\[2\\] | 3   | 2   | 6   | 24  | 40  | 0.20 | 13,056            | 5,462,148  | 4,940,136  | 10,402,284      |\n",
    "| $15^{2}\\times40$                         | MBConvBlock\\[3\\] | 3   | 1   | 6   | 40  | 40  | 0.20 | 35,040            | 5,207,904  | 4,873,800  | 10,081,704      |\n",
    "| $15^{2}\\times40$                         | MBConvBlock\\[4\\] | 3   | 1   | 6   | 40  | 48  | 0.20 | 35,088            | 5,639,904  | 5,304,000  | 10,943,904      |\n",
    "| $15^{2}\\times48$                         | MBConvBlock\\[5\\] | 3   | 1   | 6   | 48  | 64  | 0.20 | 49,632            | 8,328,267  | 7,923,744  | 16,252,011      |\n",
    "| $15^{2}\\times64$                         | MBConvBlock\\[6\\] | 3   | 1   | 6   | 64  | 64  | 0.20 | 86,784            | 12,501,348 | 11,966,784 | 24,468,132      |\n",
    "| $15^{2}\\times64$                         | MBConvBlock\\[7\\] | 3   | 2   | 6   | 64  | 80  | 0.20 | 86,880            | 7,598,436  | 7,277,104  | 14,875,540      |\n",
    "| $7^{2}\\times80$                          | MBConvBlock\\[8\\] | 3   | 1   | 6   | 80  | 80  | 0.20 | 135,360           | 4,233,408  | 4,086,160  | 8,319,568       |\n",
    "| $7^{2}\\times80$                          | MBConvBlock\\[9\\] | 3   | 1   | 6   | 80  | 96  | 0.20 | 135,456           | 4,609,728  | 4,461,696  | 9,071,424       |\n",
    "| $7^{2}\\times96$                          | Head\\_Conv2d     | 1   | 1   | -   | 96  | 136 | -    | 13,056            | 659,736    | 639,744    | 1,299,480       |\n",
    "| $7^{2}\\times136$                         | AveragePool      | 7   | -   | -   | -   | -   | -    | 0                 | 136        | 6,528      | 6,664           |\n",
    "| $136$                                    | FullyConnected   | -   | -   | -   | -   | -   | _    | 13,600            | 13,600     | 13,500     | 27,100          |\n",
    "| $100$                                    | -                | -   | -   | -   | -   | -   | -    | -                 | -          | -          | -               |\n",
    "| $15^{2}\\times40$                         | EarlyExiting     | -   | -   | -   | -   | -   | -    | 30,920            | 3,055,440  | 2,909,780  | 5,965,220       |\n",
    "| Total                                    | -                | -   | -   | -   | -   | -   | -    | 642,136           | 63,853,343 | 60,203,448 | 124,056,791     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98477644\n"
     ]
    }
   ],
   "source": [
    "earlyExitingOps = 11907+1314648+1253588+9763617+10402284+10081704+5965220\n",
    "notEarlyExitingOps = 124056791\n",
    "\n",
    "earlyExitingRatio = 0.3\n",
    "print (int(earlyExitingRatio*earlyExitingOps+(1-earlyExitingRatio)*notEarlyExitingOps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Parameter Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upsample(nearest): $0$\n",
    "* Stem_Conv2d: $3 \\times 3 \\times 3 \\times 24$ (Conv2d) + $0$ (BatchNorm) + $0$ (Activation) = $648$\n",
    "* MBConvBlock(k,s,e,i,o,se): $1 \\times 1 \\times i \\times (i \\times e)$ (Expansion_Conv) + $k \\times k \\times (i \\times e)$ (Depthwise_Conv) + $1 \\times 1 \\times (i \\times e) \\times (i \\times e \\times se)$ (SE_squeeze) + $1 \\times 1 \\times (i \\times e \\times se) \\times (i \\times e)$ (SE_expand) +  $1 \\times 1 \\times e \\times o$ (Projection_conv)\n",
    "    * when $e=1$, Expansion_Conv is omitted. (i.e., $- 1 \\times 1 \\times i \\times (i \\times e)$)\n",
    "* Head_Conv2d: $1 \\times 1 \\times 96 \\times 136$ (Conv2d) + $0$ (BatchNorm) + $0$ (Activation) = $13,056$\n",
    "* AveragePool: $0$\n",
    "* FullyConnected:$136 \\times 100$ = $13,600$\n",
    "* Early Exisiting: $1 \\times 1 \\times i \\times 2i$ (Pointwise_Conv) + $3 \\times 3 \\times 2i$ (Depthwise_Conv) + $1 \\times 1 \\times 2i \\times 150$ (Projection_Conv) + $150 \\times 100$ (Fully connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642136\n"
     ]
    }
   ],
   "source": [
    "params = 0\n",
    "\n",
    "# Stem_Conv\n",
    "params += 3*3*3*24\n",
    "\n",
    "blocks = [[3, 1, 1, 24, 16, 0.20],\n",
    "          [3, 1, 6, 16, 24, 0.20],\n",
    "          [3, 2, 6, 24, 40, 0.20],\n",
    "          [3, 1, 6, 40, 40, 0.20],\n",
    "          [3, 1, 6, 40, 48, 0.20],\n",
    "          [3, 1, 6, 48, 64, 0.20],\n",
    "          [3, 1, 6, 64, 64, 0.20],\n",
    "          [3, 2, 6, 64, 80, 0.20],\n",
    "          [3, 1, 6, 80, 80, 0.20],\n",
    "          [3, 1, 6, 80, 96, 0.20]]\n",
    "\n",
    "for k, s, e, i, o, se in blocks:\n",
    "    block_params = 0\n",
    "    if e!=1:\n",
    "        block_params += 1*1*i*(i*e)\n",
    "    block_params += k*k*(i*e)\n",
    "    block_params += 1*1*(i*e)*int(i*e*se)\n",
    "    block_params += 1*1*int(i*e*se)*(i*e)\n",
    "    block_params += 1*1*e*o\n",
    "    params += block_params\n",
    "params += 1*1*96*136\n",
    "params += 0\n",
    "params += 136*100\n",
    "\n",
    "early_exiting_in_channels = 40\n",
    "early_exiting_out_channels = 150\n",
    "params += 1*1*early_exiting_in_channels*(2*early_exiting_in_channels) + 3*3*(2*early_exiting_in_channels) +\\\n",
    "          1*1*(2*early_exiting_in_channels)*early_exiting_out_channels + early_exiting_out_channels*100\n",
    "\n",
    "print (params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Math Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Upsample(nearest): $63\\times63\\times3=11,907$\n",
    "* Stem_Conv2d: $3 \\times 3 \\times 3 \\times 24$ (Conv2d) + $0$ (BatchNorm) + $0$ (Activation) = $648$\n",
    "* MBConvBlock(k,s,e,i,o,se): $(1 \\times 1 \\times i) \\times (w_{in} \\times h_{in} \\times (i \\times e))$\n",
    "    * when $e=1$, Expansion_Conv is omitted. (i.e., $- 1 \\times 1 \\times i \\times (i \\times e)$)\n",
    "* Head_Conv2d:\n",
    "* AveragePool:\n",
    "* FullyConnected:\n",
    "* Early Exisiting:\n",
    "    * When early exiting happens,\n",
    "    * When early exiting does not happen, early exiting + 뒷부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63853343 60203448\n"
     ]
    }
   ],
   "source": [
    "total_mul_ops = 0\n",
    "total_add_ops = 0\n",
    "\n",
    "# Upsample\n",
    "total_mul_ops += 63*63*3\n",
    "total_add_ops += 0\n",
    "\n",
    "# Stem_Conv (Conv2d)\n",
    "tmp_mul_ops = (3*3*3)*(31*31*24)\n",
    "tmp_add_ops = (3*3*3-1)*(31*31*24)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "\n",
    "# Stem_Conv (Swish Activation)\n",
    "tmp_mul_ops = (3)*(31*31*24)\n",
    "tmp_add_ops = (1)*(31*31*24)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "\n",
    "blocks = [[31, 31, 3, 1, 1, 24, 16, 0.20],\n",
    "          [31, 31, 3, 1, 6, 16, 24, 0.20],\n",
    "          [31, 15, 3, 2, 6, 24, 40, 0.20],\n",
    "          [15, 15, 3, 1, 6, 40, 40, 0.20],\n",
    "          [15, 15, 3, 1, 6, 40, 48, 0.20],\n",
    "          [15, 15, 3, 1, 6, 48, 64, 0.20],\n",
    "          [15, 15, 3, 1, 6, 64, 64, 0.20],\n",
    "          [15,  7, 3, 2, 6, 64, 80, 0.20],\n",
    "          [ 7,  7, 3, 1, 6, 80, 80, 0.20],\n",
    "          [ 7,  7, 3, 1, 6, 80, 96, 0.20]]\n",
    "\n",
    "for idx, (input_size, output_size, k, s, e, i, o, se) in enumerate(blocks):\n",
    "    mul_ops = 0\n",
    "    add_ops = 0\n",
    "    \n",
    "    # Expansion\n",
    "    if e!=1:\n",
    "        tmp_mul_ops = (1*1*i)*(input_size*input_size*(i*e))\n",
    "        tmp_add_ops = (1*1*i-1)*(input_size*input_size*(i*e))\n",
    "        mul_ops += tmp_mul_ops\n",
    "        add_ops += tmp_add_ops\n",
    "    # Swish Activation\n",
    "    if e!=1:\n",
    "        tmp_mul_ops = (3)*(input_size*input_size*(i*e))\n",
    "        tmp_add_ops = (1)*(input_size*input_size*(i*e))\n",
    "        mul_ops += tmp_mul_ops\n",
    "        add_ops += tmp_add_ops\n",
    "    # Depthwise\n",
    "    tmp_mul_ops = (k*k)*(output_size*output_size*(i*e))\n",
    "    tmp_add_ops = (k*k-1)*(output_size*output_size*(i*e))\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops\n",
    "    # Swish Activation\n",
    "    tmp_mul_ops = (3)*(output_size*output_size*(i*e))\n",
    "    tmp_add_ops = (1)*(output_size*output_size*(i*e))\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops\n",
    "    \n",
    "    # SE module (GlobalAvgPool)\n",
    "    tmp_mul_ops = i*e\n",
    "    tmp_add_ops = (output_size*output_size-1)*(i*e)\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops\n",
    "    # SE module (SE_squeeze)\n",
    "    tmp_mul_ops = (1*1*(i*e))*(1*1*int(i*e*se))\n",
    "    tmp_add_ops = (1*1*(i*e)-1)*(1*1*int(i*e*se))\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops\n",
    "    # SE module (Swish Activation)\n",
    "    tmp_mul_ops = (3)*(1*1*int(i*e*se))\n",
    "    tmp_add_ops = (1)*(1*1*int(i*e*se))\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops\n",
    "    # SE module (SE_expand)\n",
    "    tmp_mul_ops = (1*1*int(i*e*se))*(1*1*(i*e))\n",
    "    tmp_add_ops = (1*1*int(i*e*se)-1)*(1*1*(i*e))\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops\n",
    "    # SE module (Sigmoid)\n",
    "    tmp_mul_ops = (2)*(1*1*(i*e))\n",
    "    tmp_add_ops = (1)*(1*1*(i*e))\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops\n",
    "    \n",
    "    # Channel-wise product\n",
    "    tmp_mul_ops = output_size*output_size*(i*e)\n",
    "    tmp_add_ops = 0\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops  \n",
    "    # Projection\n",
    "    tmp_mul_ops = (1*1*(i*e))*(output_size*output_size*o)\n",
    "    tmp_add_ops = (1*1*(i*e)-1)*(output_size*output_size*o)\n",
    "    mul_ops += tmp_mul_ops\n",
    "    add_ops += tmp_add_ops  \n",
    "    \n",
    "    total_mul_ops += mul_ops\n",
    "    total_add_ops += add_ops\n",
    "    \n",
    "# Head_Conv (Conv2d)\n",
    "tmp_mul_ops = (1*1*96)*(7*7*136)\n",
    "tmp_add_ops = (1*1*96-1)*(7*7*136)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "\n",
    "# Head_Conv (Swish Activation)\n",
    "tmp_mul_ops = (3)*(7*7*136)\n",
    "tmp_add_ops = (1)*(7*7*136)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "\n",
    "# GlobalAvgPool\n",
    "tmp_mul_ops = 136\n",
    "tmp_add_ops = (7*7-1)*136\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "\n",
    "# FullyConnected\n",
    "tmp_mul_ops = 136*100\n",
    "tmp_add_ops = (136-1)*100\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "\n",
    "# EarlyExiting (Pointwise)\n",
    "tmp_mul_ops = (1*1*40)*(15*15*80)\n",
    "tmp_add_ops = (1*1*40-1)*(15*15*80)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "# EarlyExiting (Swish Activation)\n",
    "tmp_mul_ops = (3)*(15*15*80)\n",
    "tmp_add_ops = (1)*(15*15*80)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "# EarlyExiting (Depthwise)\n",
    "tmp_mul_ops = (3*3)*(13*13*80)\n",
    "tmp_add_ops = (3*3-1)*(13*13*80)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "# EarlyExiting (Swish Activation)\n",
    "tmp_mul_ops = (3)*(13*13*80)\n",
    "tmp_add_ops = (1)*(13*13*80)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "# EarlyExiting (Projection)\n",
    "tmp_mul_ops = (1*1*80)*(13*13*150)\n",
    "tmp_add_ops = (1*1*80-1)*(13*13*150)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "# EarlyExiting (Swish Activation)\n",
    "tmp_mul_ops = (3)*(13*13*150)\n",
    "tmp_add_ops = (1)*(13*13*150)\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "# EarlyExiting (GlobalAvgPool)\n",
    "tmp_mul_ops = 150\n",
    "tmp_add_ops = (13*13-1)*150\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "# EarlyExiting (FullyConnected)\n",
    "tmp_mul_ops = 150*100\n",
    "tmp_add_ops = (150-1)*100\n",
    "total_mul_ops += tmp_mul_ops\n",
    "total_add_ops += tmp_add_ops\n",
    "\n",
    "print (total_mul_ops, total_add_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reproduce Process (random seed 고정해야하겠네) : 코드 집중\n",
    "To enhance Trainhandler에 대한 개괄적인 설명을 하고 주석으로 설명되어 있음 디테일한 내용은. We divide our reproduce process into three steps:\n",
    "1. Training the main network (~`Section 2.2`)\n",
    "    * `python main.py Config/main.json`\n",
    "2. Pruning the main network (~`Section 2.3`)\n",
    "3. Training the early exiting module with the main model frozen (~`Section 2.4`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. More details\n",
    "* zero validation set: valid를 위하여 투자를 하지 않고 train으로 모두 이용하는 것이 최종 train accuracy에 도움이 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(+) 작은 model에서 다양한 compression 방법들의 실패기록"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
